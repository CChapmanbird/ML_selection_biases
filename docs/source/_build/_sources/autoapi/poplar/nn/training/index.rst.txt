:orphan:

:py:mod:`poplar.nn.training`
============================

.. py:module:: poplar.nn.training


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   poplar.nn.training.train
   poplar.nn.training.train_test_split



.. py:function:: train(model: poplar.nn.networks.LinearModel, data: list, n_epochs: int, n_batches: int, loss_function: Any, optimiser=None, verbose=False, plot=True, update_every=1, n_test_batches=None, save_best=False, scheduler=None, outdir='models')

   Train/test loop for an instance of LinearModel. This function allows for some basic monitoring of the training process, including regular loss curve plots
   and a command line output indicating current progress.

   If you need more complex functionality, it is advisable to write your own training function using this as a starting point.

   :param model: Instance of LinearModel to be trained.
   :type model: LinearModel
   :param data: List of torch.Tensors for the training data, training labels, testing data and testing labels respectively.
   :type data: list
   :param n_epochs: Number of epochs to train for.
   :type n_epochs: int
   :param n_batches: Number of batches per epoch.
   :type n_batches: int
   :param loss_function: Loss function to use. It is recommended to use one of the pytorch loss functions (https://pytorch.org/docs/stable/nn.html#loss-functions)
   :type loss_function: Any
   :param optimiser: The pytorch optimiser to use, by default the Adam optimiser with a learning rate of 1e-3 is used. This should be instantiated before passing to this function.
   :type optimiser: Any, optional
   :param verbose: If True, also displays training progress on the command line. By default False
   :type verbose: bool, optional
   :param plot: If True, loss curves are regularly produced (with interval update_every) and saved in the model directory, by default True
   :type plot: bool, optional
   :param update_every: Number of epochs between updating the saved model and plotting diagnostic data, by default 1
   :type update_every: int, optional
   :param n_test_batches: Number of batches to run the testing data in, by default n_batches
   :type n_test_batches: int, optional
   :param save_best: If True, saves the network that achieved the lowest validation losses, by default False
   :type save_best: bool, optional
   :param scheduler: pytorch scheduler to use for learning rate adjustment during training, by default None
   :type scheduler: Any, optional
   :param outdir: Output directory for the trained model directory, by default 'models'
   :type outdir: str, optional


.. py:function:: train_test_split(data, ratio: int, device='cpu', dtype=torch.float64)

   Splits `data` into two instances of `torch.tensor` with sizes of ratio `ratio` along their first axis. Also supports device
   switching and dtype casting.

   :param data: The data to be split.
   :type data: torch.tensor or numpy.ndarray
   :param ratio: The ratio between the sizes of the two output tensors along their first axis.
   :type ratio: int
   :param device: device to move tensors to, by default "cpu"
   :type device: str, optional
   :param dtype: data type of output tensors, by default torch.float64
   :type dtype: optional

   :returns: A list of the two split tensors.
   :rtype: list of tensors


